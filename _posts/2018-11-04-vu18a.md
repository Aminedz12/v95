---
title: Batch Normalized Deep Boltzmann Machines
abstract: 'Training Deep Boltzmann Machines (DBMs) is a challenging task in deep generative
  model studies. The careless training usually leads to a divergence or a useless
  model. We discover that this phenomenon is due to the change of DBM layers’ input
  signals during model parameter updates, similar to other deterministic deep networks
  such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs).
  The change of layers’ input distributions not only complicates the learning process
  but also causes redundant neurons that simply imitate the others’ behaviors. Although
  this phenomenon can be coped using batch normalization in deep learning, integrating
  this technique into the probabilistic network of DBMs is a challenging problem since
  it has to satisfy two conditions of energy function and conditional probabilities.
  In this paper, we introduce Batch Normalized Deep Boltzmann Machines (BNDBMs) that
  meet both aforementioned conditions and successfully combine batch normalization
  and DBMs into the same framework. However, unlike CNNs, due to the probabilistic
  nature of DBMs, training DBMs with batch normalization has some differences: i)
  fixing shift parameters $\bnshift$ but learning scale parameters $\bnscale$; ii)
  avoiding normalizing the first hidden layer and iii) maintaining multiple pairs
  of population means and variances per neuron rather than one pair in CNNs. We observe
  that our proposed BNDBMs can stabilize the input signals of network layers and facilitate
  the training process as well as improve the model quality. More interestingly, BNDBMs
  can be trained successfully without pretraining, which is usually a mandatory step
  in most existing DBMs. The experimental results in MNIST, Fashion-MNIST and Caltech
  101 Silhouette datasets show that our BNDBMs outperform DBMs and centered DBMs in
  terms of feature representation and classification accuracy ($3.98%$ and $5.84%$
  average improvement for pretraining and no pretraining respectively).'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: vu18a
month: 0
tex_title: Batch Normalized Deep Boltzmann Machines
firstpage: 359
lastpage: 374
page: 359-374
order: 359
cycles: false
bibtex_author: Vu, Hung and Nguyen, {Tu Dinh} and Le, Trung and Luo, Wei and Phung,
  Dinh
author:
- given: Hung
  family: Vu
- given: Tu Dinh
  family: Nguyen
- given: Trung
  family: Le
- given: Wei
  family: Luo
- given: Dinh
  family: Phung
date: 2018-11-04
address: 
publisher: PMLR
container-title: Proceedings of The 10th Asian Conference on Machine Learning
volume: '95'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 11
  - 4
pdf: http://proceedings.mlr.press/v95/vu18a/vu18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v95/vu18a/vu18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
