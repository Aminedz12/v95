---
title: 'TVT: Two-View Transformer Network for Video Captioning'
abstract: 'Video captioning is a task of automatically generating the natural text
  description of a given video. There are two main challenges in video captioning
  under the context of an encoder-decoder framework: 1) How to model the sequential
  information; 2) How to combine the modalities including video and text. For challenge
  1), the recurrent neural networks (RNNs) based methods are currently the most common
  approaches for learning temporal representations of videos, while they suffer from
  a high computational cost. For challenge 2), the features of different modalities
  are often roughly concatenated together without insightful discussion. In this paper,
  we introduce a novel video captioning framework, i.e., Two-View Transformer (TVT).
  TVT comprises of a backbone of Transformer network for sequential representation
  and two types of fusion blocks in decoder layers for combining different modalities
  effectively. Empirical study shows that our TVT model outperforms the state-of-the-art
  methods on the MSVD dataset and achieves a competitive performance on the MSR-VTT
  dataset under four common metrics.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: chen18b
month: 0
tex_title: 'TVT: Two-View Transformer Network for Video Captioning'
firstpage: 847
lastpage: 862
page: 847-862
order: 847
cycles: false
bibtex_author: Chen, Ming and Li, Yingming and Zhang, Zhongfei and Huang, Siyu
author:
- given: Ming
  family: Chen
- given: Yingming
  family: Li
- given: Zhongfei
  family: Zhang
- given: Siyu
  family: Huang
date: 2018-11-04
address: 
publisher: PMLR
container-title: Proceedings of The 10th Asian Conference on Machine Learning
volume: '95'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 11
  - 4
pdf: http://proceedings.mlr.press/v95/chen18b/chen18b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
