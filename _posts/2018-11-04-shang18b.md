---
title: 'ASVRG: Accelerated Proximal SVRG'
abstract: This paper proposes an accelerated proximal stochastic variance reduced
  gradient (ASVRG) method, in which we design a simple and effective momentum acceleration
  trick. Unlike most existing accelerated stochastic variance reduction methods such
  as Katyusha, ASVRG has only one additional variable and one momentum parameter.
  Thus, ASVRG is much simpler than those methods, and has much lower per-iteration
  complexity. We prove that ASVRG achieves the best known oracle complexities for
  both strongly convex and non-strongly convex objectives. In addition, we extend
  ASVRG to mini-batch and non-smooth settings. We also empirically verify our theoretical
  results and show that the performance of ASVRG is comparable with, and sometimes
  even better than that of the state-of-the-art stochastic methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shang18b
month: 0
tex_title: 'ASVRG: Accelerated Proximal SVRG'
firstpage: 815
lastpage: 830
page: 815-830
order: 815
cycles: false
bibtex_author: Shang, Fanhua and Jiao, Licheng and Zhou, Kaiwen and Cheng, James and
  Ren, Yan and Jin, Yufei
author:
- given: Fanhua
  family: Shang
- given: Licheng
  family: Jiao
- given: Kaiwen
  family: Zhou
- given: James
  family: Cheng
- given: Yan
  family: Ren
- given: Yufei
  family: Jin
date: 2018-11-04
address: 
publisher: PMLR
container-title: Proceedings of The 10th Asian Conference on Machine Learning
volume: '95'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 11
  - 4
pdf: http://proceedings.mlr.press/v95/shang18b/shang18b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v95/shang18b/shang18b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
